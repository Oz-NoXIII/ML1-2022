{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4_Exercises",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Last week, we learned how to fit a simple model to a single or multiple predictors to predict a given target variable. However, by fixing the number of predictors and by fixing the model's hyper-parameters beforehand, we restricted ourselves to a single model. By doing so, we omitted to explore a broader range of models, one of which might better explain to relationship between our input and target variables.**\n",
        "\n",
        "**In this lab, we'll experiment with the general methodology of model selection, meaning that we'll define a set of predifined models, and we'll retain the one that minimizes the out-of-sample error.**"
      ],
      "metadata": {
        "id": "H3g2zh1W2t_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First, import the necessary libraries**"
      ],
      "metadata": {
        "id": "1U5VE1l3YtKA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkt8eCSmYkbz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, cross_validate, LeaveOneOut, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_selection import SequentialFeatureSelector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) In this lab, we will be working with the 'fish_lab.csv' dataset, which contains several characteristics about fish, such as their weights, lengths, and species, among others. Load the dataset as a pandas dataframe, inspect its properties and check for any missing values. Change the data type of 'Species' to 'category'.**"
      ],
      "metadata": {
        "id": "I_O6yz5QY8bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TO0Fesd7Zp0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Does the dataframe contain any missing values ? If yes, replace the missing values by the sample mean for continuous variables. For missing categorical variables, replace them by the most frequent occurence of the corresponding column. Check the 'SimpleImputer()' class of the scikit.learn library.**"
      ],
      "metadata": {
        "id": "ARI4jGJDZ24R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X6p-MsWdagTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Unfortunately, categorical variables cannot be fed directly to the model. One approach to deal with the problem is to create 'dummy variables' out of the categorical one, where each dummy variable takes the value '1' if the observation belongs to that class, and '0' otherwise. Check the method 'get_dummies' of the pandas library to generate such dummy variables.** "
      ],
      "metadata": {
        "id": "jmndzfo-anRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "da0wVt_ibvcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Now let's make some predictions. Select the variable 'Weight' as predictor and 'Height' as the target variable. Generate a scatter plot of the two variables, and fit a simple linear regression model to the data. Split your datasets into a training and test set following a 60/40 partition. Evaluate the model on the Mean Squared Error and on the coefficent of determination. Report such metrics for both the training and test sets.**"
      ],
      "metadata": {
        "id": "SNR5NKrlbxbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZAQqipascyCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) By doing a 60/40 partition for our training and test sets, we are unfortunately neglecting a significant portion of data that could come in handy for training our model. This is where cross-validation becomes highly useful, as it allows us to train on a greater proportion of the total dataset.**\n",
        "\n",
        "**Use cross-validation to train the model. Check the performance of the model for both a 10 folds cross-validation, and a leave-one-out cross-validation. In both cases, evaluate the models on the MSE. Report the mean and the variance of the test MSE across each folds. What do you observe ? Check the classes 'cross_validate()' and 'LeaveOneOut() of the scikit-learn library.**"
      ],
      "metadata": {
        "id": "SQivk8PmdDND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mr3iWrn3eHfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SgYminmhfaGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6) Maybe a linear model is not the best option to model the relationship between the variables 'Weight' and 'Height'. Instead of fitting a linear model, let's fit a polynomial model of specified degree. First, transform the 'Weight' feature to quadratic, and fit a simple linear regression model to predict the variable 'Height' using a 10 folds cross-validation, and evaluate on the MSE. Report the mean test MSE across each folds. Check the method PolynomialFeatures() to transform the input variable.**\n"
      ],
      "metadata": {
        "id": "U_PEwHV0f14c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o1a-6zXngJgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7) Let's now see how the MSE vary if we increase the model's complexity. For polynomial degrees between 1 and 20, repeatedly fit the model using a simple linear regression model and cross-validation, and plot the evolution of the mean train and test MSE in function of the polynomial degree. What do you observe ? Use the function 'PolynomialRegression' as the 'model' argument of the class 'cross_validate', and make sure to understand what the 'make_pipeline' method does.**\n",
        "\n",
        "**Moreover, can you identify which degree would be the best choice to model the relationship between the variables 'Weigth' and 'Height' ?**"
      ],
      "metadata": {
        "id": "aZm20YCJjb2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PolynomialRegression(degree=2, **kwargs):\n",
        "    return make_pipeline(PolynomialFeatures(degree),\n",
        "                         LinearRegression(**kwargs))"
      ],
      "metadata": {
        "id": "QwZ91HOXjpEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZpaKtxAwk_Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8) Maybe selecting more than one feature might also result in a better fit. Use a forward selection strategy to select to best subset of 3 predictors for predicting the target variable 'Height'. See the class 'SequentialFeaturesSelector()' of the scikit-learn library. Use the scorer below as a scoring function.**\n",
        "\n",
        "**Report the selected variables, and fit a multiple linear regression model to predict the variable 'Height' using a 10 folds cross-validation. Report the mean MSE across each folds.** "
      ],
      "metadata": {
        "id": "zdqIb5hmlgf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-vRZNmkel8j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9) Usually, models have more than one hyper-parameter that be tuned in order to find the model that best captures the relationship between our input and target variables. For instance, in the case of a simple linear regression using a polynomial transformation on the input variables, the hyper-parameter space would be the polymial's degree, and whether or not to fit the intercept. Inspecting each combination of of hyper-parameters and selecting the combination that results in the best model is called Grid Search. However, manually inspecting each combination could be a very tedious task, but fortunatly, scikit-learn provides a class GridSearchCV() that implements this protocol for you.**\n",
        "\n",
        "**Select 'Weight' as predictor and 'Height' as target variable, and perform a Grid Search on the hyper-parameters space of a simple linear regression using a polynomial transformation of input variable. Search for degrees varying between 1 and 20, and whether or not the intercept should be fit. Report the best subset of hyper-parameters, as well as the test MSE for a model fitted using this subset. Plot the obtained regression curve**"
      ],
      "metadata": {
        "id": "s0qZ8rGFpBKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BJJ0uaGlrtAk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}